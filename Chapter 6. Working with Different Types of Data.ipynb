{"cells":[{"cell_type":"code","source":["df= spark.read.format('csv') \\\n         .option('header','true')\\\n         .option('inferschema','true')\\\n         .load('dbfs:/FileStore/tables/employeetable.csv')\ndf.printSchema()\ndf.createOrReplaceTempView('dftable')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- empid : integer (nullable = true)\n-- empname : string (nullable = true)\n-- location : string (nullable = true)\n-- salary: integer (nullable = true)\n\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf.show()\ndf.where(col('salary') != 3005)\\\n  .select('salary','location ')\\\n  .show(5,False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------------+----------+------+\nempid |    empname | location |salary|\n+------+------------+----------+------+\n   100|karthikeyan |   chennai|  1003|\n   200| navaneethan|coimbatore|  2004|\n   300|      nalini|   madurai|  3005|\n   400|     prakash|coimbatore|  4006|\n   500| nivedhithaa|  dindugal|  5007|\n+------+------------+----------+------+\n\n+------+----------+\nsalary|location  |\n+------+----------+\n1003  |chennai   |\n2004  |coimbatore|\n4006  |coimbatore|\n5007  |dindugal  |\n+------+----------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql import Row\nfrom pyspark.sql.functions import instr\ndata =[\n      Row('amma amma appa','anna','akka'),\n      Row('appa appa appa','akka','anna'),\n      Row('appa','amma','anna')\n]   \nrdd = sc.parallelize(data)\ndf = spark.createDataFrame(rdd,['desc','rela','main'])\ndf5 =df.select(col('desc'))\ndf5.where(instr(df5.desc,'amma') ==1 ).show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+\n          desc|\n+--------------+\namma amma appa|\n+--------------+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["df = spark.createDataFrame([('abcd',)], ['s',])\ndf.select(instr(df.s,'c')).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+\ninstr(s, c)|\n+-----------+\n          3|\n+-----------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["df= spark.read.format('csv') \\\n         .option('header','true')\\\n         .option('inferschema','true')\\\n         .load('dbfs:/FileStore/tables/employeetable.csv')\ndf.printSchema()\ndf.createOrReplaceTempView('dftable')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- empid : integer (nullable = true)\n-- empname : string (nullable = true)\n-- location : string (nullable = true)\n-- salary: integer (nullable = true)\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------------+----------+------+\nempid |    empname | location |salary|\n+------+------------+----------+------+\n   100|karthikeyan |   chennai|  1003|\n   200| navaneethan|coimbatore|  2004|\n   300|      nalini|   madurai|  3005|\n   400|     prakash|coimbatore|  4006|\n   500| nivedhithaa|  dindugal|  5007|\n+------+------------+----------+------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import expr,pow\npow1 = pow(col('empid ')*col('salary'),2)+5\ndf.select(pow1).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------------+\n(POWER((empid  * salary), 2.0) + 5)|\n+-----------------------------------+\n                    1.0060090005E10|\n                   1.60640640005E11|\n                   8.12702250005E11|\n                  2.567685760005E12|\n                  6.267512250005E12|\n+-----------------------------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["df.describe().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------------------+------------+---------+------------------+\nsummary|            empid |    empname |location |            salary|\n+-------+------------------+------------+---------+------------------+\n  count|                 5|           5|        5|                 5|\n   mean|             300.0|        null|     null|            3005.0|\n stddev|158.11388300841898|        null|     null|1582.7199689142737|\n    min|               100|karthikeyan |  chennai|              1003|\n    max|               500|     prakash|  madurai|              5007|\n+-------+------------------+------------+---------+------------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.functions import monotonically_increasing_id \ndf.select(monotonically_increasing_id()).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------+\nmonotonically_increasing_id()|\n+-----------------------------+\n                            0|\n                            1|\n+-----------------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------------+----------+------+\nempid |    empname | location |salary|\n+------+------------+----------+------+\n   100|karthikeyan |   chennai|  1003|\n   200| navaneethan|coimbatore|  2004|\n   300|      nalini|   madurai|  3005|\n   400|     prakash|coimbatore|  4006|\n   500| nivedhithaa|  dindugal|  5007|\n+------+------------+----------+------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["from pyspark.sql.functions import initcap,lower,upper\ndf.select(initcap(col('empname ')),lower(col('empname ')),upper(col('empname '))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+---------------+---------------+\ninitcap(empname )|lower(empname )|upper(empname )|\n+-----------------+---------------+---------------+\n     Karthikeyan |   karthikeyan |   KARTHIKEYAN |\n      Navaneethan|    navaneethan|    NAVANEETHAN|\n           Nalini|         nalini|         NALINI|\n          Prakash|        prakash|        PRAKASH|\n      Nivedhithaa|    nivedhithaa|    NIVEDHITHAA|\n+-----------------+---------------+---------------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["  df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------------+----------+------+\nempid |    empname | location |salary|\n+------+------------+----------+------+\n   100|karthikeyan |   chennai|  1003|\n   200| navaneethan|coimbatore|  2004|\n   300|      nalini|   madurai|  3005|\n   400|     prakash|coimbatore|  4006|\n   500| nivedhithaa|  dindugal|  5007|\n+------+------------+----------+------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_extract\nextract_str = \"(nalini|nivedhithaa)\"\ndf.select(regexp_extract(col('empname '),extract_str,1)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------------------------------+\nregexp_extract(empname , (nalini|nivedhithaa), 1)|\n+-------------------------------------------------+\n                                                 |\n                                                 |\n                                           nalini|\n                                                 |\n                                      nivedhithaa|\n+-------------------------------------------------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql import Row\nfrom pyspark.sql.functions import instr\nfrom pyspark.sql.functions import regexp_extract\ndata =[\n      Row('amma amma appa','anna','akka'),\n      Row('appa amma appa','akka','anna'),\n      Row('appa appa amma','amma','anna')\n]   \nrdd = sc.parallelize(data)\ndf = spark.createDataFrame(rdd,['desc','rela','main'])\nextract_str = \"(amma)\"\ndf.select(regexp_extract('desc',extract_str,1),'main').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------------+----+\nregexp_extract(desc, (amma), 1)|main|\n+-------------------------------+----+\n                           amma|akka|\n                           amma|anna|\n                           amma|anna|\n+-------------------------------+----+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["#Reqular expressions\nimport re\nstr = \"the rain in Spain\"\nx = re.findall(\"ai\",str)\nprint(x)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&apos;ai&apos;, &apos;ai&apos;]\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["import re\nstr = \"amma appa anna akka amma where are you currenlty am working with cognizant \"\ns = re.findall(\"amma\",str)\nprint(s)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&apos;amma&apos;, &apos;amma&apos;]\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["# Search \nimport re \nstr = \"The rain is Spain\"\nx = re.search(\"is\",str)\nprint(x.start())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">9\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["str = \"The rain in Spain\"\nx = re.split(\"\\s\", str)\nprint(x)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&apos;The&apos;, &apos;rain&apos;, &apos;in&apos;, &apos;Spain&apos;]\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["#current_date,current_timestamp\nfrom pyspark.sql.functions import current_date, current_timestamp \ndataDF = spark.range(10)\\\n              .withColumn(\"today\",current_date())\\\n              .withColumn(\"now\",current_timestamp())\ndataDF.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+--------------------+\n id|     today|                 now|\n+---+----------+--------------------+\n  0|2019-05-20|2019-05-20 03:06:...|\n  1|2019-05-20|2019-05-20 03:06:...|\n  2|2019-05-20|2019-05-20 03:06:...|\n  3|2019-05-20|2019-05-20 03:06:...|\n  4|2019-05-20|2019-05-20 03:06:...|\n  5|2019-05-20|2019-05-20 03:06:...|\n  6|2019-05-20|2019-05-20 03:06:...|\n  7|2019-05-20|2019-05-20 03:06:...|\n  8|2019-05-20|2019-05-20 03:06:...|\n  9|2019-05-20|2019-05-20 03:06:...|\n+---+----------+--------------------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["#date_add,date_sup\nfrom pyspark.sql.functions import date_add,date_sub,datediff\ndataDF.select(datediff(date_add(\"today\",5),\"today\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------------+\ndatediff(date_add(today, 5), today)|\n+-----------------------------------+\n                                  5|\n                                  5|\n                                  5|\n                                  5|\n                                  5|\n                                  5|\n                                  5|\n                                  5|\n                                  5|\n                                  5|\n+-----------------------------------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["dataDF.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+--------------------+\n id|     today|                 now|\n+---+----------+--------------------+\n  0|2019-05-20|2019-05-20 03:46:...|\n  1|2019-05-20|2019-05-20 03:46:...|\n  2|2019-05-20|2019-05-20 03:46:...|\n  3|2019-05-20|2019-05-20 03:46:...|\n  4|2019-05-20|2019-05-20 03:46:...|\n  5|2019-05-20|2019-05-20 03:46:...|\n  6|2019-05-20|2019-05-20 03:46:...|\n  7|2019-05-20|2019-05-20 03:46:...|\n  8|2019-05-20|2019-05-20 03:46:...|\n  9|2019-05-20|2019-05-20 03:46:...|\n+---+----------+--------------------+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["#woking with complex Types \ndataDF.show()\ncomplexDF = dataDF."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+--------------------+\n id|     today|                 now|\n+---+----------+--------------------+\n  0|2019-05-20|2019-05-20 03:48:...|\n  1|2019-05-20|2019-05-20 03:48:...|\n  2|2019-05-20|2019-05-20 03:48:...|\n  3|2019-05-20|2019-05-20 03:48:...|\n  4|2019-05-20|2019-05-20 03:48:...|\n  5|2019-05-20|2019-05-20 03:48:...|\n  6|2019-05-20|2019-05-20 03:48:...|\n  7|2019-05-20|2019-05-20 03:48:...|\n  8|2019-05-20|2019-05-20 03:48:...|\n  9|2019-05-20|2019-05-20 03:48:...|\n+---+----------+--------------------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["from pyspark.sql.functions import struct\ncomplexDF = dataDF.select(struct(\"id\",\"today\").alias(\"complext\"))\ncomplexDF.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+\n       complext|\n+---------------+\n[0, 2019-05-20]|\n[1, 2019-05-20]|\n[2, 2019-05-20]|\n[3, 2019-05-20]|\n[4, 2019-05-20]|\n[5, 2019-05-20]|\n[6, 2019-05-20]|\n[7, 2019-05-20]|\n[8, 2019-05-20]|\n[9, 2019-05-20]|\n+---------------+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["complexDF.select(\"complext.id\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n id|\n+---+\n  0|\n  1|\n  2|\n  3|\n  4|\n  5|\n  6|\n  7|\n  8|\n  9|\n+---+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["complexDF.select(\"complex\".getField(\"id\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-95433901563549&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>complexDF<span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;complex&quot;</span><span class=\"ansiyellow\">.</span>getField<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;id&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;str&apos; object has no attribute &apos;getField&apos;</div>"]}}],"execution_count":25},{"cell_type":"code","source":["#Working on One UseCase \n\n#Step-01 Declaring the Lib\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import split\ndata = [ Row('Karthikeyan Navaneethan Nalini','coimbatore',1000)\n         ,Row('Prakash Ajayanth Aswanth','Chennai',2000)\n\t     ,Row('Sendhan Seyon','Rasipalayam',3000)\n            ]\nrdd = sc.parallelize(data)\n\n#Step-02 Creating the dataFrame \n\ndf_usecase = spark.createDataFrame(rdd,['name','location','salary'])\n\n#Step-03 Split the column value \n\ndf_usecase_split = df_usecase.select(split(\"name\",\" \").alias(\"complexdata\"))\n\ndf_usecase_split.show()\n\n#Step-04: \n\ndf_usecase_split.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n         complexdata|\n+--------------------+\n[Karthikeyan, Nav...|\n[Prakash, Ajayant...|\n    [Sendhan, Seyon]|\n+--------------------+\n\nroot\n-- complexdata: array (nullable = true)\n    |-- element: string (containsNull = true)\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["from pyspark.sql.functions import size,explode\ndf1 = df_usecase.select(split(\"name\",\" \").alias(\"complexdata\"))\ndf2 = df1.withColumn(\"exploded\",explode(\"complexdata\")).show()\n        \n         "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-----------+\n         complexdata|   exploded|\n+--------------------+-----------+\n[Karthikeyan, Nav...|Karthikeyan|\n[Karthikeyan, Nav...|Navaneethan|\n[Karthikeyan, Nav...|     Nalini|\n[Prakash, Ajayant...|    Prakash|\n[Prakash, Ajayant...|   Ajayanth|\n[Prakash, Ajayant...|    Aswanth|\n    [Sendhan, Seyon]|    Sendhan|\n    [Sendhan, Seyon]|      Seyon|\n+--------------------+-----------+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"name":"Chapter 6. Working with Different Types of Data","notebookId":470586028341045},"nbformat":4,"nbformat_minor":0}
