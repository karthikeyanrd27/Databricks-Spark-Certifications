{"cells":[{"cell_type":"code","source":["from pyspark import HiveContext\nfrom pyspark import SQLContext \na = [10,20,30,40,50]\nrdd = sc.parallelize(a)\nrdd1 = rdd.map(lambda x : x)\nrdd1.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">2</span><span class=\"ansired\">]: </span>[10, 20, 30, 40, 50]\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["rdd1.toDebugString()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">5</span><span class=\"ansired\">]: </span>b&apos;(8) PythonRDD[2] at RDD at PythonRDD.scala:56 []\\n |  ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:267 []&apos;\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["'''\n\n1. Concepts of Action\n\nAction[1] --> Job[1] --> Stages[n] --> Tasks[n]\n\n- New Job is created based on the actions is performed \n-New stages will be created based on the if there is data shuffle is required . Data suffle is nothing but when required  save the output between the nodes \n- New tasks will be created based on the number of partition  in RDD \n\n'''\n\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext \nrdd = sc.textFile(\"/FileStore/tables/sfo_weather.csv\").option(\"header\" = True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;command-1558806534081857&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">15</span>\n<span class=\"ansiyellow\">    rdd = sc.textFile(&quot;/FileStore/tables/sfo_weather.csv&quot;).option(&quot;header&quot; = True)</span>\n<span class=\"ansigrey\">                                                                 ^</span>\n<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> keyword can&apos;t be an expression\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["from pyspark import HiveContext \nfrom pyspark import SQLContext\nsc=SQLContext(sc)\ndf= sc.read.format('com.databricks.spark.csv').options(header = 'true',inferschema='true').load('/FileStore/tables/flight_delays1.csv')\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"08_Job_Stages_Tasks","notebookId":3531555324754799},"nbformat":4,"nbformat_minor":0}
