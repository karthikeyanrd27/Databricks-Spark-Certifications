{"cells":[{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext \nrdd = sc.parallelize(range(1,10))\nrdd.collect()\npair_rdd = rdd.map(lambda x :(x,1))\npair_rdd.collect()\npair_rdd.getNumPartitions()\ngroup_rdd = pair_rdd.groupByKey()\ngroup_rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">10</span><span class=\"ansired\">]: </span>8\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["df = spark.createDataFrame(pair_rdd)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":3}],"metadata":{"name":"03_Shuffle_partitions","notebookId":1429083793487603},"nbformat":4,"nbformat_minor":0}
