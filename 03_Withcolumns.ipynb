{"cells":[{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql import Row, functions as F\ndf = spark.read.csv('dbfs:/FileStore/tables/employeetable.csv',header = True)\ndf.withColumn('sub_salary', \\\n              F.when((F.col('salary')%1000 ==3),'Fizz')\\\n               .when(F.col('salary')%1000 ==5,'buzz')\\\n               .when(F.col('salary')%1000 == 4 ,'Fizzbuzz')\\\n               .otherwise(F.col('salary'))).show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------------+----------+------+----------+\nempid |    empname | location |salary|sub_salary|\n+------+------------+----------+------+----------+\n   100|karthikeyan |   chennai|  1003|      Fizz|\n   200| navaneethan|coimbatore|  2004|  Fizzbuzz|\n   300|      nalini|   madurai|  3005|      buzz|\n   400|     prakash|coimbatore|  4006|      4006|\n   500| nivedhithaa|  dindugal|  5007|      5007|\n+------+------------+----------+------+----------+\n\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\ndf = spark.read.csv('dbfs:/FileStore/tables/employeetable.csv',header = True)\ndf.withColumn('salary_trans',\\\n              F.when(F.col('salary')%1000 ==3,'fizz')\\\n               .when(F.col('salary')%1000 ==5,'buzz')\\\n               .when(F.col('salary')%1000 ==6,'fizzbuzz')\n               .otherwise(F.col('salary'))\n             ).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------------+----------+------+------------+\nempid |    empname | location |salary|salary_trans|\n+------+------------+----------+------+------------+\n   100|karthikeyan |   chennai|  1003|        fizz|\n   200| navaneethan|coimbatore|  2004|        2004|\n   300|      nalini|   madurai|  3005|        buzz|\n   400|     prakash|coimbatore|  4006|    fizzbuzz|\n   500| nivedhithaa|  dindugal|  5007|        5007|\n+------+------------+----------+------+------------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql import Row, functions as F\ndf = spark.read.csv('dbfs:/FileStore/tables/employeetable.csv',header = True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["from pyspark import HiveContext\nfrom pyspark import SQLContext\ndf = spark.read.format('csv').load('dbfs:/FileStore/tables/employeetable.csv',header = True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------------+----------+------+\nempid |    empname | location |salary|\n+------+------------+----------+------+\n   100|karthikeyan |   chennai|  1003|\n   200| navaneethan|coimbatore|  2004|\n   300|      nalini|   madurai|  3005|\n   400|     prakash|coimbatore|  4006|\n   500| nivedhithaa|  dindugal|  5007|\n+------+------------+----------+------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":6}],"metadata":{"name":"03_Withcolumns","notebookId":2465098488309400},"nbformat":4,"nbformat_minor":0}
