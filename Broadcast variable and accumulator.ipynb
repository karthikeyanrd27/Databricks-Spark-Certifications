{"cells":[{"cell_type":"code","source":["'''\nLearning : accumaltor and broadcast variables are shared variables \nAccumulator - aggrigating the value from worker node and back to the driver program \n'''"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\ndef v(x):\n    global cnt\n    if (x == 100):\n        cnt += 1\n    return x\ncnt = sc.accumulator(0)\na = [100,200,300,100,100,100,100]\nrdd = sc.parallelize(a)\nrdd1 = rdd.map(v)\nrdd1.collect()\nprint (cnt)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">5\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\ndef v(x):\n  global cnt\n  if (x == 100 ):\n     cnt +=1\n  return cnt\ncnt = sc.accumulator(4)\na = [100,100,100,200]\nrdd = sc.parallelize(a)\nrdd1 = rdd.map(v)\nrdd1.collect()\n\n\nprint(cnt)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">7\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["''' Learning: \n1.Accumulator are write-only variables for executors.They can be added to by executors and read by the driver only.\n'''"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["broadcastVar = sc.broadcast([1, 2, 3])\nbroadcastVar.value"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">26</span><span class=\"ansired\">]: </span>[1, 2, 3]\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["a = [100,100,100,200]\nrdd = sc.parallelize(a)\nrdd.max()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">28</span><span class=\"ansired\">]: </span>200\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["rdd.min()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">29</span><span class=\"ansired\">]: </span>100\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["rdd.stdev()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">30</span><span class=\"ansired\">]: </span>43.301270189221931\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["'''When we are broadcasting large values, it is important to choose a data serialization format that is both fast and compact, because the time to send the value over the network can quickly become a bottleneck if it takes a long time to either serialize a value or to send the serialized value over the network. In particular, Java Serialization, the default serialization library used in Sparkâ€™s Scala and Java APIs, can be very inefficient out of the box for anything except arrays of primitive types. You can optimize serialization by selecting a different serialization library using the spark.serializer property '''"],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"name":"Broadcast variable and accumulator","notebookId":2681075426807589},"nbformat":4,"nbformat_minor":0}
