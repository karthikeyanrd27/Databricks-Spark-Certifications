{"cells":[{"cell_type":"code","source":["# Dataframe without Schema\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\ntemp_list = [('Karthikeyan',37),('Navaneethan',41),('Nalini',46)]\ndf = spark.createDataFrame(temp_list)\ndf.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">5</span><span class=\"ansired\">]: </span>\n[Row(_1=&apos;Karthikeyan&apos;, _2=37),\n Row(_1=&apos;Navaneethan&apos;, _2=41),\n Row(_1=&apos;Nalini&apos;, _2=46)]\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["# Dataframe with Schema\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\ntemp_list = [('karthikeyan',37),('navaneethan',40),('nalini',35)]\ndf = spark.createDataFrame(temp_list,['name','age'])\ndf.collect()\ndf.select('name').collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>[Row(name=&apos;karthikeyan&apos;), Row(name=&apos;navaneethan&apos;), Row(name=&apos;nalini&apos;)]\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Dataframe with Schema based on the RDD\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\ntmp_list = [('Karthikeyan',37),('navaneethan',45),('nalini',45)]\nrdd = sc.parallelize(tmp_list)\ndf = spark.createDataFrame(rdd)\ndf.printSchema()\ndf.select('_1').collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _1: string (nullable = true)\n-- _2: long (nullable = true)\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">16</span><span class=\"ansired\">]: </span>[Row(_1=&apos;Karthikeyan&apos;), Row(_1=&apos;navaneethan&apos;), Row(_1=&apos;nalini&apos;)]\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Dataframe with Schema based on the RDD with Schema\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\ntmp_list = [('karthikeyan',37),('nalini',46)]\nschema_list = ['name','age']\nrdd = sc.parallelize(tmp_list)\ndf = spark.createDataFrame(rdd,schema_list)\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+\n       name|age|\n+-----------+---+\nkarthikeyan| 37|\n     nalini| 46|\n+-----------+---+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Dataframe with Struct Based on the Struct\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql.types import *\ntemp_list = [('karthikeyan',35),('navaneethan',45)]\nrdd = sc.parallelize(temp_list)\nschema = StructType([\n                   StructField('name',StringType()),\n                   StructField('age',StringType())\n])\ndf =spark.createDataFrame(rdd,schema)\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+\n       name|age|\n+-----------+---+\nkarthikeyan| 35|\nnavaneethan| 45|\n+-----------+---+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Dataframe select with aggrigation \nfrom pyspark import HiveContext\nfrom pyspark import SQLContext\nfrom pyspark.sql.types import*\ntmp_list = [('karthikeyan',37),('navaneethan',45)]\nrdd = sc.parallelize(tmp_list)\nschema = StructType([\n                    StructField('name',StringType()),\n                    StructField('age',StringType())\n])\ndf = spark.createDataFrame(rdd,schema)\ndf.agg({\n  'age':'max'\n}).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\nmax(age)|\n+--------+\n      45|\n+--------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Dataframe select with aggrigation sum \nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\ntmp_list = [('karthikeyan',20),('nalini',45)]\nrdd = sc.parallelize(tmp_list)\nschema = StructType([\n                    StructField('name',StringType()),\n                    StructField('age',StringType())\n])\ndf = spark.createDataFrame(rdd,schema)\ndf.agg({'age':'sum'}).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">35</span><span class=\"ansired\">]: </span>[Row(sum(age)=65.0)]\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["df.agg({'age':'avg'}).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">36</span><span class=\"ansired\">]: </span>[Row(avg(age)=32.5)]\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["df.agg({'age':'min'}).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">37</span><span class=\"ansired\">]: </span>[Row(min(age)=&apos;20&apos;)]\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark import HiveContext\nfrom pyspark import SQLContext \nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\ntmp_list = [('karthikeyan',27)]\nrdd = sc.parallelize(tmp_list)\nshchema = StructType([\n                     StructField('name',StringType()),\n                     StructField('age',StringType())\n])\ndf = spark.createDataFrame(rdd,shchema)\nmin_val = df.agg(F.min('age'))\nmin_val.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">43</span><span class=\"ansired\">]: </span>[Row(min(age)=&apos;27&apos;)]\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\ntmp_emp = [(100,'karthikeyan'),(200,'navaneethan'),(300,'nalini')]\ntmp_dep = [(100,'ece'),(200,'chemical')]\ndf_emp =spark.createDataFrame(tmp_emp,['depid','name'])\ndf_dep =spark.createDataFrame(tmp_dep,['depid','dep_name'])\ndf_join = df_emp.join(df_dep,'depid','inner')\ndf_join.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----------+--------+\ndepid|       name|dep_name|\n+-----+-----------+--------+\n  100|karthikeyan|     ece|\n  200|navaneethan|chemical|\n+-----+-----------+--------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["df_jon1 = df_emp.alias('emp').join(df_dep.alias('dep'),'depid','inner')\ndf_jon1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----------+--------+\ndepid|       name|dep_name|\n+-----+-----------+--------+\n  100|karthikeyan|     ece|\n  200|navaneethan|chemical|\n+-----+-----------+--------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql.functions import *\ntmp_list = [('karthikeyan',34)]\nrdd = sc.parallelize(tmp_list)\nrdd.collect()\ndf = spark.createDataFrame(rdd,['name','age'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["df.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-2284076889718392&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>df<span class=\"ansiyellow\">.</span>getNumPartitions<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansicyan\">__getattr__</span><span class=\"ansiblue\">(self, name)</span>\n<span class=\"ansigreen\">   1323</span>         <span class=\"ansigreen\">if</span> name <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">in</span> self<span class=\"ansiyellow\">.</span>columns<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1324</span>             raise AttributeError(\n<span class=\"ansigreen\">-&gt; 1325</span><span class=\"ansiyellow\">                 &quot;&apos;%s&apos; object has no attribute &apos;%s&apos;&quot; % (self.__class__.__name__, name))\n</span><span class=\"ansigreen\">   1326</span>         jc <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>apply<span class=\"ansiyellow\">(</span>name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1327</span>         <span class=\"ansigreen\">return</span> Column<span class=\"ansiyellow\">(</span>jc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;DataFrame&apos; object has no attribute &apos;getNumPartitions&apos;</div>"]}}],"execution_count":14},{"cell_type":"code","source":["rdd1= df.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["df.coalesce(2).rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">64</span><span class=\"ansired\">]: </span>2\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["Folloing API Need learn:\ncorr\ncolRegex(colName)[source]\napproxQuantile(col, probabilities, relativeError)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-2284076889718395&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> corr<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>colRegex<span class=\"ansiyellow\">(</span>colName<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span>source<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> approxQuantile<span class=\"ansiyellow\">(</span>col<span class=\"ansiyellow\">,</span> probabilities<span class=\"ansiyellow\">,</span> relativeError<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;colRegex&apos; is not defined</div>"]}}],"execution_count":17},{"cell_type":"code","source":["df.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">67</span><span class=\"ansired\">]: </span>[Row(Col1=&apos;a&apos;, Col2=1), Row(Col1=&apos;b&apos;, Col2=2), Row(Col1=&apos;c&apos;, Col2=3)]\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["from pyspark import HiveContext\nfrom pyspark import SQLContext\ntmp_list = [('karthikeyan',34)]\ndf=spark.createDataFrame(tmp_list,['name','age'])\ndf.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">73</span><span class=\"ansired\">]: </span>[&apos;name&apos;, &apos;age&apos;]\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# Dataframe Cross Joins\nfrom pyspark import HiveContext\nfrom pyspark import SQLContext\ntmp_emp = [(100,'karthikeyan',10),(200,'navaneethan',20),(300,'nalini',20)]\ntmp_dep = [(100,'it'),(200,'ece'),(300,'eee')]\nrdd1 = sc.parallelize(tmp_emp)\nrdd2 = sc.parallelize(tmp_dep)\ndf_emp = spark.createDataFrame(rdd1,['empid','name','depid'])\ndf_dep = spark.createDataFrame(rdd2,['depid','depname'])\ndf = df_emp.crossJoin(df_dep)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["df_emp.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----------+-----+\nempid|       name|depid|\n+-----+-----------+-----+\n  100|karthikeyan|   10|\n  200|navaneethan|   20|\n  300|     nalini|   20|\n+-----+-----------+-----+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["df_dep.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-------+\ndepid|depname|\n+-----+-------+\n  100|     it|\n  200|    ece|\n  300|    eee|\n+-----+-------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----------+-----+-----+-------+\nempid|       name|depid|depid|depname|\n+-----+-----------+-----+-----+-------+\n  100|karthikeyan|   10|  100|     it|\n  100|karthikeyan|   10|  200|    ece|\n  100|karthikeyan|   10|  300|    eee|\n  200|navaneethan|   20|  100|     it|\n  200|navaneethan|   20|  200|    ece|\n  200|navaneethan|   20|  300|    eee|\n  300|     nalini|   20|  100|     it|\n  300|     nalini|   20|  200|    ece|\n  300|     nalini|   20|  300|    eee|\n+-----+-----------+-----+-----+-------+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- empid: long (nullable = true)\n-- name: string (nullable = true)\n-- depid: long (nullable = true)\n-- depid: long (nullable = true)\n-- depname: string (nullable = true)\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["df.crosstab('empid','name').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------+------+-----------+\nempid_name|karthikeyan|nalini|navaneethan|\n+----------+-----------+------+-----------+\n       200|          0|     0|          3|\n       100|          3|     0|          0|\n       300|          0|     3|          0|\n+----------+-----------+------+-----------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["# Dataframe Cube\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\nemp_lst = [('karthikeyan',45),('nalini',67),('navaneethan',69)]\nrdd = sc.parallelize(emp_lst)\ndf = spark.createDataFrame(rdd,['name','age'])\ndf.cube('name',df.age).count().show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----+-----+\n       name| age|count|\n+-----------+----+-----+\n     nalini|  67|    1|\nkarthikeyan|  45|    1|\n       null|  67|    1|\nkarthikeyan|null|    1|\n       null|null|    3|\nnavaneethan|null|    1|\nnavaneethan|  69|    1|\n     nalini|null|    1|\n       null|  69|    1|\n       null|  45|    1|\n+-----------+----+-----+\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+\n       name|age|\n+-----------+---+\nkarthikeyan| 45|\n     nalini| 67|\nnavaneethan| 69|\n+-----------+---+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["df.describe(['age','name']).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------------------+-----------+\nsummary|               age|       name|\n+-------+------------------+-----------+\n  count|                 3|          3|\n   mean|60.333333333333336|       null|\n stddev|13.316656236958785|       null|\n    min|                45|karthikeyan|\n    max|                69|navaneethan|\n+-------+------------------+-----------+\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["df.distinct().collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">100</span><span class=\"ansired\">]: </span>\n[Row(name=&apos;navaneethan&apos;, age=69),\n Row(name=&apos;karthikeyan&apos;, age=45),\n Row(name=&apos;nalini&apos;, age=67)]\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["from pyspark import HiveContext\nfrom pyspark import SQLContext\nemp_list = [('karthikeyan',34,100000),('navaneethan',45,30000),('nalini',56,6777)]\nrdd = sc.parallelize(emp_list)\ndf = spark.createDataFrame(rdd,['name','age','salary'])\ndf.drop('age').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+------+\n       name|salary|\n+-----------+------+\nkarthikeyan|100000|\nnavaneethan| 30000|\n     nalini|  6777|\n+-----------+------+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["from pyspark import HiveContext\nfrom pyspark import SQLContext\nemp_list = [('karthikeyan',10),('nalini',20),('navaneethan',10)]\ndep_list = [(10,'it'),(20,'ece')]\nrdd_emp = sc.parallelize(emp_list)\nrdd_dep = sc.parallelize(dep_list)\ndf_emp = spark.createDataFrame(rdd_emp,['name','depid'])\ndf_dep = spark.createDataFrame(rdd_dep,['depid','depname'])\ndf_join = df_emp.join(df_dep,'depid','inner')\ndf_join.drop('name').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-------+\ndepid|depname|\n+-----+-------+\n   10|     it|\n   10|     it|\n   20|    ece|\n+-----+-------+\n\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["df.drop('age').collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">119</span><span class=\"ansired\">]: </span>\n[Row(name=&apos;karthikeyan&apos;, salary=100000),\n Row(name=&apos;navaneethan&apos;, salary=30000),\n Row(name=&apos;nalini&apos;, salary=6777)]\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["df.drop(df.age).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">120</span><span class=\"ansired\">]: </span>\n[Row(name=&apos;karthikeyan&apos;, salary=100000),\n Row(name=&apos;navaneethan&apos;, salary=30000),\n Row(name=&apos;nalini&apos;, salary=6777)]\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\nemp_list = [('karthikeyan',37,'coimbatore'),\n            ('karthikeyan',37,'coimbatore'),\n            ('karthikeyan',45,'coimbatore')\n           ]\nrdd = sc.parallelize(emp_list)\ndf = spark.createDataFrame(rdd,['name','age','location'])\ndf1 =df.dropDuplicates()\ndf1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+----------+\n       name|age|  location|\n+-----------+---+----------+\nkarthikeyan| 37|coimbatore|\nkarthikeyan| 45|coimbatore|\n+-----------+---+----------+\n\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["df.dropDuplicates(['age']).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">132</span><span class=\"ansired\">]: </span>\n[Row(name=&apos;karthikeyan&apos;, age=37, location=&apos;coimbatore&apos;),\n Row(name=&apos;karthikeyan&apos;, age=45, location=&apos;coimbatore&apos;)]\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["df.drop_duplicates().collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">135</span><span class=\"ansired\">]: </span>\n[Row(name=&apos;karthikeyan&apos;, age=37, location=&apos;coimbatore&apos;),\n Row(name=&apos;karthikeyan&apos;, age=45, location=&apos;coimbatore&apos;)]\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["df.drop_duplicates().collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">137</span><span class=\"ansired\">]: </span>\n[Row(name=&apos;karthikeyan&apos;, age=37, location=&apos;coimbatore&apos;),\n Row(name=&apos;karthikeyan&apos;, age=45, location=&apos;coimbatore&apos;)]\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\ndf = spark.createDataFrame([(1, 'Karthikeyan', 1.79, 28,'M', 'Tiler'),\n                            (2, 'Navaneethan', 1.78, 45,'M', None),\n                            (3, 'Nalini', 1.75, None, None, None),\n                            (4, 'Prakash',1.6, 33,'F', 'Dancer'),\n                            (5, 'Gregory', 1.8, 54,'M', 'Teacher'),\n                            (6, 'Steven', 1.82, None, 'M', None),\n                            (7, 'Dagmar', 1.7, 42,'F', 'Nurse'),]\n                           , ['id', 'Name', 'Height', 'Age', 'Gender', 'Occupation'])\ndf.show()\ndf1 = df.dropna(thresh=2,subset=('Age','Gender'))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----------+------+----+------+----------+\n id|       Name|Height| Age|Gender|Occupation|\n+---+-----------+------+----+------+----------+\n  1|Karthikeyan|  1.79|  28|     M|     Tiler|\n  2|Navaneethan|  1.78|  45|     M|      null|\n  3|     Nalini|  1.75|null|  null|      null|\n  4|    Prakash|   1.6|  33|     F|    Dancer|\n  5|    Gregory|   1.8|  54|     M|   Teacher|\n  6|     Steven|  1.82|null|     M|      null|\n  7|     Dagmar|   1.7|  42|     F|     Nurse|\n+---+-----------+------+----+------+----------+\n\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["df1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----------+------+---+------+----------+\n id|       Name|Height|Age|Gender|Occupation|\n+---+-----------+------+---+------+----------+\n  1|Karthikeyan|  1.79| 28|     M|     Tiler|\n  2|Navaneethan|  1.78| 45|     M|      null|\n  4|    Prakash|   1.6| 33|     F|    Dancer|\n  5|    Gregory|   1.8| 54|     M|   Teacher|\n  7|     Dagmar|   1.7| 42|     F|     Nurse|\n+---+-----------+------+---+------+----------+\n\n</div>"]}}],"execution_count":39},{"cell_type":"code","source":["from pyspark import HiveContext\nfrom pyspark import SQLContext\nemp_list = [('karthikeyan',45),('navaneethan',45)]\nrdd = sc.parallelize(emp_list)\ndf = spark.createDataFrame(rdd,['name','age'])\ndf.dtypes"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">5</span><span class=\"ansired\">]: </span>[(&apos;name&apos;, &apos;string&apos;), (&apos;age&apos;, &apos;bigint&apos;)]\n</div>"]}}],"execution_count":40},{"cell_type":"code","source":["''' SQL EXCEPT operator takes the distinct rows of one query and returns the rows that do not appear in a second result set. The EXCEPT ALL operator does not remove duplicates. For purposes of row elimination and duplicate removal, the EXCEPT operator does not distinguish between NULLs'''\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql.types import *\nemp_list =[('karthikeyan',37),('navaneethan',45),('nalini',56)]\nrdd_emp = sc.parallelize(emp_list)\nschema = StructType([\n                    StructField('name',StringType()),\n                    StructField('age',StringType())\n])\ndf_emp =spark.createDataFrame(rdd_emp,schema)\ndf_emp1 = spark.createDataFrame([('karthikeyan',32),('navaneethan',45),('nalini',56)],['name','age'])\ndiff = df_emp.exceptAll(df_emp1)\ndiff.show()\n#If both df are identical giving the 0 result\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+\n       name|age|\n+-----------+---+\nkarthikeyan| 37|\n+-----------+---+\n\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["df.explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(1) Scan ExistingRDD[name#28,age#29]\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["from pyspark import HiveContext\nfrom pyspark import SQLContext\nfrom pyspark.sql.types import *\nemp_list =[('karthikeyan',34,''),('navaneethan',45,'')]\nschema=StructType([StructField('name',StringType()),\n                   StructField('age',StringType()),\n                   StructField('height',StringType())])\n\ndf =spark.createDataFrame(emp_list,schema)\ndf_fill = df.fillna('null',subset=('height'))\ndf_fill.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+------+\n       name|age|height|\n+-----------+---+------+\nkarthikeyan| 34|      |\nnavaneethan| 45|      |\n+-----------+---+------+\n\n</div>"]}}],"execution_count":43},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql import functions as F\ndf_filter = df.filter(F.col('age')==45)\ndf_filter.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+------+\n       name|age|height|\n+-----------+---+------+\nnavaneethan| 45|      |\n+-----------+---+------+\n\n</div>"]}}],"execution_count":44},{"cell_type":"code","source":["df.first()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">39</span><span class=\"ansired\">]: </span>Row(name=&apos;karthikeyan&apos;, age=&apos;34&apos;, height=&apos;&apos;)\n</div>"]}}],"execution_count":45},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext \nemp_list =[('karthikeyan',45),('nalini',45),('amma',56),('prakash',67)]\ndf=spark.createDataFrame(emp_list,['name','age'])\ndf.groupby().avg().collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">42</span><span class=\"ansired\">]: </span>[Row(avg(age)=53.25)]\n</div>"]}}],"execution_count":48},{"cell_type":"code","source":["'''\nIf your data is sorted using either sort() or ORDER BY, these operations will be deterministic and return either the 1st element using first()/head() or the top-n using head(n)/take(n).\n\nshow()/show(n) return Unit (void) and will print up to the first 20 rows in a tabular form.\n\nThese operations may require a shuffle if there are any aggregations, joins, or sorts in the underlying query.\n\nUnsorted Data\n\nIf the data is not sorted, these operations are not guaranteed to return the 1st or top-n elements - and a shuffle may not be required.\n\nshow()/show(n) return Unit (void) and will print up to 20 rows in a tabular form and in no particular order.\n\nIf no shuffle is required (no aggregations, joins, or sorts), these operations will be optimized to inspect enough partitions to satisfy the operation - likely a much smaller subset of the overall partitions of the dataset\n'''\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\nemp_list=[('karthikeyan',56),('nalini',89),('navaneethan',90)]\ndf = spark.createDataFrame(emp_list,['name','age'])\ndf.head(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">45</span><span class=\"ansired\">]: </span>\n[Row(name=&apos;karthikeyan&apos;, age=56),\n Row(name=&apos;nalini&apos;, age=89),\n Row(name=&apos;navaneethan&apos;, age=90)]\n</div>"]}}],"execution_count":49},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\nemp_list = [('karthikeyan',45),('navaneethan',45),('nalini',60)]\ndep_list = [(45,'it'),(55,'ece')]\ndf_emp = spark.createDataFrame(emp_list,['name','depid'])\ndf_dep = spark.createDataFrame(dep_list,['depid','depname'])\ndf_join = df_emp.join(df_dep.hint('broadcast'),'depid','inner')\ndf_join.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----------+-------+\ndepid|       name|depname|\n+-----+-----------+-------+\n   45|karthikeyan|     it|\n   45|navaneethan|     it|\n+-----+-----------+-------+\n\n</div>"]}}],"execution_count":50},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext \na = [10,20,30,40]\nb = [10,50,60,20]\nrdd_a = sc.parallelize(a)\nrdd_b = sc.parallelize(b)\ndf1 = rdd_a.map(lambda x: (x, )).toDF()\ndf2 = rdd_b.map(lambda x: (x, )).toDF()\ndf1.show()\ndf2.show()\ndf1.intersect(df2).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n _1|\n+---+\n 10|\n 20|\n 30|\n 40|\n+---+\n\n+---+\n _1|\n+---+\n 10|\n 50|\n 60|\n 20|\n+---+\n\n+---+\n _1|\n+---+\n 10|\n 20|\n+---+\n\n</div>"]}}],"execution_count":51},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\nemp_lst = [('karthikeyan',34,1000),('navaneethan',45,5000),('nalini',45,6000),('prakash',34,5000)]\nemp_lst1 = [('karthikeyan','RI')]\nschema1 = ['name','age','sal']\nschema2 = ['name','st']\ndf =spark.createDataFrame(emp_lst,schema1)\ndf1 = spark.createDataFrame(emp_lst1,schema2)\ndf_join = df.join(df1,df.name == df1.name,'full_outer').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+----+-----------+----+\n       name|age| sal|       name|  st|\n+-----------+---+----+-----------+----+\nkarthikeyan| 34|1000|karthikeyan|  RI|\n     nalini| 45|6000|       null|null|\n    prakash| 34|5000|       null|null|\nnavaneethan| 45|5000|       null|null|\n+-----------+---+----+-----------+----+\n\n</div>"]}}],"execution_count":52},{"cell_type":"code","source":[" df=[('karthikeyan',34,1000),\n  ('navaneethan',45,5000),\n  ('nalini',45,6000),\n  ('prakash',34,5000)]\n  df1=[('karthikeyan','RI')]\n  \nInner Join:\n  \n  +-----------+---+----+-----------+---+\n|       name|age| sal|       name| st|\n+-----------+---+----+-----------+---+\n|karthikeyan| 34|1000|karthikeyan| RI|\n+-----------+---+----+-----------+---+\n\nOuter Join: \n+-----------+---+----+-----------+----+\n|       name|age| sal|       name|  st|\n+-----------+---+----+-----------+----+\n|karthikeyan| 34|1000|karthikeyan|  RI|\n|     nalini| 45|6000|       null|null|\n|    prakash| 34|5000|       null|null|\n|navaneethan| 45|5000|       null|null|\n+-----------+---+----+-----------+----+\n\n\n"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["# Create DF without inferSchema()\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\ntmp_list_schema = [('A',1),('B',2)]\ndf = spark.createDataFrame(tmp_list_schema,['id','no'])\ndf.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">3</span><span class=\"ansired\">]: </span>[Row(id=&apos;A&apos;, no=1), Row(id=&apos;B&apos;, no=2)]\n</div>"]}}],"execution_count":55},{"cell_type":"code","source":["# Create DF with inferSchema();{}\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\ntmp_list_schema = [{'id':'A','no':1}]\ndf=spark.createDataFrame(tmp_list_schema)\ndf.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>[Row(id=&apos;A&apos;, no=1)]\n</div>"]}}],"execution_count":56},{"cell_type":"code","source":["# Create DF Row Oject Parallelize\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext \nfrom pyspark.sql import Row\ntmp_list_noschema=[('A',1),('B',2)]\ntmp_schme = Row('id','no')\nrdd = sc.parallelize(tmp_list_noschema)\nrdd1 = rdd.map(lambda x : tmp_schme(x))\ndf = spark.createDataFrame(rdd1)\ndf.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">16</span><span class=\"ansired\">]: </span>[Row(id=Row(_1=&apos;A&apos;, _2=1)), Row(id=Row(_1=&apos;B&apos;, _2=2))]\n</div>"]}}],"execution_count":57},{"cell_type":"code","source":["# Create DF by using Panda \nimport pandas as pd \npa_df = pd.DataFrame([['A',100],['B',200]])\ndf = spark.createDataFrame(pa_df)\ndf.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">22</span><span class=\"ansired\">]: </span>[Row(0=&apos;A&apos;, 1=100), Row(0=&apos;B&apos;, 1=200)]\n</div>"]}}],"execution_count":58},{"cell_type":"code","source":["# Register Dataframe as temp view: createOrReplaceTempView\nimport pandas as pd\npa_df = pd.DataFrame([['A',10],['B',20]])\ndf =spark.createDataFrame(pa_df)\ndf.createOrReplaceTempView('testtab')\nspark.sql('select * from testtab').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+\n  0|  1|\n+---+---+\n  A| 10|\n  B| 20|\n+---+---+\n\n</div>"]}}],"execution_count":59},{"cell_type":"code","source":["# Register Dataframe as temp view: createtemptable\nimport pandas as pd \npd_df = pd.DataFrame([['A',10],['B',20]])\ndf = spark.createDataFrame(pd_df)\ndf.createOrReplaceTempView('testview')\nspark.sql('select * from testview').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+\n  0|  1|\n+---+---+\n  A| 10|\n  B| 20|\n+---+---+\n\n</div>"]}}],"execution_count":60},{"cell_type":"code","source":["# Register Dataframe as temp view: registerdataframeas table\nfrom pyspark import SQLContext\nsc =SQLContext(sc)\nimport pandas as pd\ndf_pd = pd.DataFrame([['A',10],['B',30]])\ndf = spark.createDataFrame(df_pd)\nsc.registerDataFrameAsTable(df,'test')\nspark.sql('select * from test').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+\n  0|  1|\n+---+---+\n  A| 10|\n  B| 30|\n+---+---+\n\n</div>"]}}],"execution_count":61},{"cell_type":"code","source":["# Register Dataframe UDF\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql.types import IntegerType\nsc = SQLContext(sc)\ndef strlen(x):\n    return len(x)\nsc.udf.register('strlen',strlen,IntegerType())\nsc.registerDataFrameAsTable(df,'testtab')\nspark.sql(\"select strlen('karthikeyan')\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+\nstrlen(karthikeyan)|\n+-------------------+\n                 11|\n+-------------------+\n\n</div>"]}}],"execution_count":62},{"cell_type":"code","source":["# Register with UDF\nfrom pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql.types import IntegerType\ndef strlen(x):\n    return len(x)\nsc= SQLContext(sc)\nspark.udf.register('strlen',strlen,IntegerType())\nspark.sql(\"select strlen('karthikeyan')\").show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+\nstrlen(karthikeyan)|\n+-------------------+\n                 11|\n+-------------------+\n\n</div>"]}}],"execution_count":63},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\nimport pandas as pd \npd_df = pd.DataFrame([['a',10],['b',20],['c',40],['d',50]])\ndf = spark.createDataFrame(pd_df,['ID','NO'])\ndf.show()\ndf.groupBy().avg().show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+\n ID| NO|\n+---+---+\n  a| 10|\n  b| 20|\n  c| 40|\n  d| 50|\n+---+---+\n\n+-------+\navg(NO)|\n+-------+\n   30.0|\n+-------+\n\n</div>"]}}],"execution_count":64},{"cell_type":"code","source":["df.groupBy(\"ID\").pivot(\"ID\").sum(\"NO\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+----+----+----+\n ID|   a|   b|   c|   d|\n+---+----+----+----+----+\n  d|null|null|null|  50|\n  c|null|null|  40|null|\n  b|null|  20|null|null|\n  a|  10|null|null|null|\n+---+----+----+----+----+\n\n</div>"]}}],"execution_count":65},{"cell_type":"code","source":["df.groupBy(\"ID\").pivot(\"ID\").sum(\"NO\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+----+----+----+\n ID|   a|   b|   c|   d|\n+---+----+----+----+----+\n  d|null|null|null|  50|\n  c|null|null|  40|null|\n  b|null|  20|null|null|\n  a|  10|null|null|null|\n+---+----+----+----+----+\n\n</div>"]}}],"execution_count":66},{"cell_type":"code","source":["df.groupBy('ID').pivot('ID').sum('NO').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+----+----+----+\n ID|   a|   b|   c|   d|\n+---+----+----+----+----+\n  d|null|null|null|  50|\n  c|null|null|  40|null|\n  b|null|  20|null|null|\n  a|  10|null|null|null|\n+---+----+----+----+----+\n\n</div>"]}}],"execution_count":67},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\ndata = [('Karthikeyan',100),('Navaneethan',200)]\nrdd = sc.parallelize(data)\ndf = spark.createDataFrame(rdd)\ndf.withColumn('new',F.lit('Test')).show()\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+----+\n         _1| _2| new|\n+-----------+---+----+\nKarthikeyan|100|Test|\nNavaneethan|200|Test|\n+-----------+---+----+\n\n+-----------+---+\n         _1| _2|\n+-----------+---+\nKarthikeyan|100|\nNavaneethan|200|\n+-----------+---+\n\n</div>"]}}],"execution_count":68},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\na = [('a',1),('b',2),('c',3)]\nb = [('rd',1),('rn',3)]\ndf1 = spark.createDataFrame(a,['id','no'])\ndf2 = spark.createDataFrame(b,['id','no'])\ndf1.crossJoin(df2).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+---+\n id| no| id| no|\n+---+---+---+---+\n  a|  1| rd|  1|\n  a|  1| rn|  3|\n  b|  2| rd|  1|\n  b|  2| rn|  3|\n  c|  3| rd|  1|\n  c|  3| rn|  3|\n+---+---+---+---+\n\n</div>"]}}],"execution_count":69},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\ndata = [('karthikeyan',10),('navaneethan',30)]\ndf = spark.createDataFrame(data,['name','age'])\ndf.persist()\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+\n       name|age|\n+-----------+---+\nkarthikeyan| 10|\nnavaneethan| 30|\n+-----------+---+\n\n</div>"]}}],"execution_count":70},{"cell_type":"code","source":["from pyspark import SQLContext\nfrom pyspark import HiveContext\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\n\ndata = [(\"Ali\", 0, [100]),\n    (\"Barbara\", 1, [300, 250, 100]),\n    (\"Cesar\", 1, [350, 100]),\n    (\"Dongmei\", 1, [400, 100]),\n    (\"Eli\", 2, [250]),\n    (\"Florita\", 2, [500, 300, 100]),\n    (\"Gatimu\", 3, [300, 100])]\ndf = spark.createDataFrame(data,['name','department','score'])\ndf.show()\ndf1 = df.withColumn('score1',explode(col('score'))).show()\nwindow = Window.partitionBy(col('department')).orderBy(col('score1'))\n#df2 = df1.withColumn('rank1', dense_rank().over(window))\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-4464423183604273&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     17</span> <span class=\"ansired\">#df2 = df1.withColumn(&apos;rank1&apos;, dense_rank().over(window))</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     18</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 19</span><span class=\"ansiyellow\"> df1.withColumn(\n</span><span class=\"ansigreen\">     20</span>   &quot;rank&quot;, dense_rank().over(Window.partitionBy(&quot;department&quot;).orderBy(desc(&quot;score1&quot;))))\n\n<span class=\"ansired\">AttributeError</span>: &apos;NoneType&apos; object has no attribute &apos;withColumn&apos;</div>"]}}],"execution_count":71},{"cell_type":"code","source":["from pyspark.sql import Row\ndf1 = spark.createDataFrame([\n           Row(id=1, value='foo'),\n           Row(id=2, value=None)])\ndf1.select(\ndf1['value'] == 'foo',\ndf1['value'].eqNullSafe('foo'),\ndf1['value'].eqNullSafe(None)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+---------------+----------------+\n(value = foo)|(value &lt;=&gt; foo)|(value &lt;=&gt; NULL)|\n+-------------+---------------+----------------+\n         true|           true|           false|\n         null|          false|            true|\n+-------------+---------------+----------------+\n\n</div>"]}}],"execution_count":72},{"cell_type":"code","source":["df2 = spark.createDataFrame([\nRow(id=1, value=float('NaN')),\nRow(id=2, value=42.0),\nRow(id=3, value=None)\n])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":73},{"cell_type":"code","source":["df2.select(\n df2['value'].eqNullSafe(None),\n df2['value'].eqNullSafe(float('NaN')),\n df2['value'].eqNullSafe(42.0)\n).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+---------------+----------------+\n(value &lt;=&gt; NULL)|(value &lt;=&gt; NaN)|(value &lt;=&gt; 42.0)|\n+----------------+---------------+----------------+\n           false|           true|           false|\n           false|          false|            true|\n            true|          false|           false|\n+----------------+---------------+----------------+\n\n</div>"]}}],"execution_count":74},{"cell_type":"code","source":["df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----+---+-----+\n id|value| id|value|\n+---+-----+---+-----+\n  1|  foo|  3| null|\n  2| null|  3| null|\n+---+-----+---+-----+\n\n</div>"]}}],"execution_count":75},{"cell_type":"code","source":["df1.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- id: long (nullable = true)\n-- value: string (nullable = true)\n\n</div>"]}}],"execution_count":76},{"cell_type":"code","source":["df1.withColumn('id1',col('id'))\ndf1.drop(col('id'))\ndf1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1181962480339888&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>df1<span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;id1&apos;</span><span class=\"ansiyellow\">,</span>col<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;id&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> df1<span class=\"ansiyellow\">.</span>drop<span class=\"ansiyellow\">(</span>col<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;id&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> df1<span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;df1&apos; is not defined</div>"]}}],"execution_count":77},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":78}],"metadata":{"name":"Dataframe Reference","notebookId":2284076889718373},"nbformat":4,"nbformat_minor":0}
